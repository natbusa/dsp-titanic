{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scaffolding for Data Science"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After setting up various data science project and tutorials, I came up with a recipe for better productivity and automation. Here below you will find a guided tour about this data science setup."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Elements\n",
    "\n",
    "This scaffolding works with three elements, which are co-ordinated with each other:\n",
    "\n",
    "  - a project template \n",
    "  - a python package (datalabframework)\n",
    "  - configuration files (metadata.yml)\n",
    "  - continuos integration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Principles ##\n",
    "\n",
    "- ** Both notebooks and code are first citizens **\n",
    "\n",
    "In the source directory `src` you will find all source code, and static files. In particular, both notebooks and code files are treated as source files. Source code is further partitioned in in ETL, ML, Publish directory. When a certain directory contains the `__init__.py` code, it will be handled as a python module. \n",
    "\n",
    "Python notebooks and Python code can be mixed and matched, and are interoperable with each other. You can include function from a notebook to a python code, and you can include python files in a notebook. \n",
    "\n",
    "- ** Data Directories should not contain logic code **\n",
    "\n",
    "Data can be located anywhere, on remote HDFS clusters, or Object Store Services exposed via S3 protocols etc. However, in general is a good practice to keep some (or all data, if possible) locally on the file system. \n",
    "\n",
    "Local data can be used also to validate the logic written in the data science steps and to test the data science pipeline in a single node setup, before scaling the code to larger datasets on more powerful clusters.\n",
    "\n",
    "- ** Decouple Code from Configuration **\n",
    "\n",
    "Notebook and Code should be decoupled from both engine configurations and from data locations. All configuration is kept in `metadata.yml` yaml files. Multiple setups for test, exploration, production can be described  in the same `metadata.yml` file or in separate multiple files.\n",
    "\n",
    "- ** Declarative Configuration **\n",
    "\n",
    "Metadata files are responsible for the binding of data and engine configurations to the code. For instance all data in the code shouold be referenced by an alias, and storage and retrieval of data object and files should happen via a common API. The metadata yaml file, describes the providers for each data source as well as the mapping of data aliases to their corresponding data objects. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Project Template\n",
    "\n",
    "The data science project is structured in a way to facilitate the deployment of the artifacts, and to switch from batch processing to live experimentation. The top level project is composed of the following items:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Top level Structure\n",
    "\n",
    "```\n",
    "  - binder\n",
    "  - ci\n",
    "  - data\n",
    "  - src\n",
    "  Makefile\n",
    "  README.md\n",
    "```\n",
    "\n",
    "The directory `binder` contains all a descriptioon of all the packages (python, apt, etc) and the pre- and post- install scripts to run to notebooks and code in the `src` directory.  The `data` directory contains sample data for the project. While the `src` directory contains all the code, assets, and documents. Finally, `ci` contains all the scrits and the configuration for continuos development, integration, and deployment. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Directory Structure\n",
    "\n",
    "The data science project is structured in three group of directories\n",
    "\n",
    " - ETL (Extract, Transform, Load)\n",
    " - ML (Machine Learning)\n",
    " - Publish (Reference Documents, and notebooks for blogs)\n",
    "\n",
    "Next to that we have `data` which is meant to store local data. Testing directories can be optionally added under each specific subdirectory, and the `explore` directory is for everything else :)\n",
    "\n",
    "```\n",
    "  - etl\n",
    "    - extract\n",
    "    \n",
    "  - ml\n",
    "    - features  \n",
    "    - models\n",
    "      - <model_name>  \n",
    "        - train\n",
    "        - validate\n",
    "        - test\n",
    "    - apis\n",
    "\n",
    "  - data\n",
    "    - datasets\n",
    "      - raw\n",
    "      - clean\n",
    "    - models\n",
    "\n",
    "  - publish\n",
    "    - reports  \n",
    "    - assets  \n",
    "      - imgs\n",
    "      - docs\n",
    "      - js\n",
    "      - css\n",
    "  \n",
    "  - explore\n",
    "\n",
    "```\n",
    "\n",
    "**Extract**   \n",
    "All ETL related code, reusable for automation. The code in here, either notebooks or python files can run to scale by binding the code to bigger files and more powerful execution engines, if necessary. The files can be directly instantiated by a scheduling/orchestrator tools for execution and dependencies (airflow, jenkins, drone, concourse).\n",
    "  \n",
    "**Features**  \n",
    "All feature engineering, possibly reusable across multiple ML models\n",
    "  \n",
    "**Models**  \n",
    "Predictive models, Data Science, Machine Learning: both training, (cross-) validation, testing\n",
    "\n",
    "**Reports**  \n",
    "Articles for reading in digital form, either statically or as live notebooks\n",
    "\n",
    "**Explore**  \n",
    "All free format experiments go here\n",
    "  \n",
    "**Data**  \n",
    "Some (smaller) data goes here, however the metadata files can refer to data located elsewhere. See data providers.\n",
    "\n",
    "```\n",
    "  - data\n",
    "    - <run>\n",
    "      - datasets\n",
    "        - raw\n",
    "        - clean\n",
    "      - models\n",
    "        - sklearn\n",
    "        - pmml\n",
    "        - spark\n",
    "          - modelname.version.timestamp\n",
    "```  \n",
    "**Assets**  \n",
    "Pics, Static diagrams, pdf, external docs, js libraries, css files, etc etc.\n",
    "  \n",
    "**Api**  \n",
    "API's can be exposed here. For instance, using the Jupyter HTTP kernel. \n",
    "... Or Flask, Or tornado. Or your fav HTTP API library."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How to set it up\n",
    "\n",
    "A template can be instantiated using the `cookiecutter` python project."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Lab Framework"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datalabframework as dlf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Modules: project\n",
    "\n",
    "When the datalabframework is imported, it starts by searching for a `__main__.py` file, according to python module file naming conventions. All modules and alias paths are all relative to this project root path."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/jovyan/src/publish/reports/howto'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dlf.project.rootpath()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Modules: Params\n",
    "\n",
    "Configuration is declared in metadata. Metadata is accumulated starting from the rootpath, and metadata files in submodules are merged all up together."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "metadata = dlf.params.metadata()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "engines:\n",
      "  howto_numpy:\n",
      "    context: numpy\n",
      "  howto_pandas:\n",
      "    context: pandas\n",
      "  howto_spark:\n",
      "    config:\n",
      "      jobname: howto\n",
      "      master: local[*]\n",
      "    context: spark\n",
      "providers:\n",
      "  howto:\n",
      "    rootpath: data\n",
      "    service: fs\n",
      "resources:\n",
      "  .ascombe:\n",
      "    format: csv\n",
      "    path: datasets/ascombe.csv\n",
      "    provider: howto\n",
      "  .corr:\n",
      "    format: csv\n",
      "    path: datasets/correlation.csv\n",
      "    provider: howto\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# some pretty printing\n",
    "dlf.params.pretty_print(metadata)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data resources are relative to the `rootpath`. Next to the resources we can declare `providers` and `engines`. More about data binding in the next section."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Modules: Data\n",
    "\n",
    "Data binding works with the metadata files. It's a good practice to declare the actual binding in the metadata and avoiding hardcoding the paths in the notebooks and python source files."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`dlf.data.path()` maps the alias to the fully qualified path, while the `dlf.data.uri()` provide the full absolute uri, where . denotes how deep in the directory structure the data is located."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'.ascombe'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dlf.data.uri('ascombe')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/jovyan/src/publish/reports/howto/data/datasets/ascombe.csv'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#relative to the current directory\n",
    "dlf.data.path('ascombe')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/jovyan/src/publish/reports/howto/data/datasets/ascombe.csv'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#absolute alias from rootpath\n",
    "dlf.data.path('.ascombe')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Modules: Notebook\n",
    "\n",
    "This submodules contains a set of utilies to extract info from notebooks. In particular, how to get the notebook name and the statistics about the cells being run.\n",
    "\n",
    "`dlf.notebook.filename()` provides the filename of the current notebook, and the path relative to the `rootpath`. `dlf.notebook.list_all()` lists all notebooks under the given rootpath."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'howto.ipynb'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dlf.notebook.get_filename()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['howto.ipynb', 'hello.ipynb']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dlf.notebook.list_all()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Modules: Engines\n",
    "\n",
    "This submodules will allow you to start a context, from the configuration described in the metadata. It also provide, basic load/store data functions according to the aliases defined in the configuration.\n",
    "\n",
    "Let's start by listing the aliases and the configuration of the engines declared in `metadata.yml`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "howto_numpy:\n",
      "  context: numpy\n",
      "howto_pandas:\n",
      "  context: pandas\n",
      "howto_spark:\n",
      "  config:\n",
      "    jobname: howto\n",
      "    master: local[*]\n",
      "  context: spark\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# get the aliases of the engines\n",
    "\n",
    "dlf.params.pretty_print(metadata['engines'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Context: Pandas__  \n",
    "Let's start the engine session, by selecting a pandas context from the list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "engine = dlf.engines.get('howto_pandas')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Access the context by using the context method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'pandas:0.23.1'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd = engine.context()\n",
    "\n",
    "#print out name and version\n",
    "'{}:{}'.format(pd.__name__, pd.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's use the context to store and load some data. First let's create a dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Ix</th>\n",
       "      <th>Iy</th>\n",
       "      <th>IIx</th>\n",
       "      <th>IIy</th>\n",
       "      <th>IIIx</th>\n",
       "      <th>IIIy</th>\n",
       "      <th>IVx</th>\n",
       "      <th>IVy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>10.0</td>\n",
       "      <td>8.04</td>\n",
       "      <td>10.0</td>\n",
       "      <td>9.14</td>\n",
       "      <td>10.0</td>\n",
       "      <td>7.46</td>\n",
       "      <td>8.0</td>\n",
       "      <td>6.58</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>8.0</td>\n",
       "      <td>6.95</td>\n",
       "      <td>8.0</td>\n",
       "      <td>8.14</td>\n",
       "      <td>8.0</td>\n",
       "      <td>6.77</td>\n",
       "      <td>8.0</td>\n",
       "      <td>5.76</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>13.0</td>\n",
       "      <td>7.58</td>\n",
       "      <td>13.0</td>\n",
       "      <td>8.74</td>\n",
       "      <td>13.0</td>\n",
       "      <td>12.74</td>\n",
       "      <td>8.0</td>\n",
       "      <td>7.71</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>9.0</td>\n",
       "      <td>8.81</td>\n",
       "      <td>9.0</td>\n",
       "      <td>8.77</td>\n",
       "      <td>9.0</td>\n",
       "      <td>7.11</td>\n",
       "      <td>8.0</td>\n",
       "      <td>8.84</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>11.0</td>\n",
       "      <td>8.33</td>\n",
       "      <td>11.0</td>\n",
       "      <td>9.26</td>\n",
       "      <td>11.0</td>\n",
       "      <td>7.81</td>\n",
       "      <td>8.0</td>\n",
       "      <td>8.47</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>14.0</td>\n",
       "      <td>9.96</td>\n",
       "      <td>14.0</td>\n",
       "      <td>8.10</td>\n",
       "      <td>14.0</td>\n",
       "      <td>8.84</td>\n",
       "      <td>8.0</td>\n",
       "      <td>7.04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>6.0</td>\n",
       "      <td>7.24</td>\n",
       "      <td>6.0</td>\n",
       "      <td>6.13</td>\n",
       "      <td>6.0</td>\n",
       "      <td>6.08</td>\n",
       "      <td>8.0</td>\n",
       "      <td>5.25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>4.0</td>\n",
       "      <td>4.26</td>\n",
       "      <td>4.0</td>\n",
       "      <td>3.10</td>\n",
       "      <td>4.0</td>\n",
       "      <td>5.39</td>\n",
       "      <td>19.0</td>\n",
       "      <td>12.50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>12.0</td>\n",
       "      <td>10.84</td>\n",
       "      <td>12.0</td>\n",
       "      <td>9.13</td>\n",
       "      <td>12.0</td>\n",
       "      <td>8.15</td>\n",
       "      <td>8.0</td>\n",
       "      <td>5.56</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>7.0</td>\n",
       "      <td>4.82</td>\n",
       "      <td>7.0</td>\n",
       "      <td>7.26</td>\n",
       "      <td>7.0</td>\n",
       "      <td>6.42</td>\n",
       "      <td>8.0</td>\n",
       "      <td>7.91</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>5.0</td>\n",
       "      <td>5.68</td>\n",
       "      <td>5.0</td>\n",
       "      <td>4.74</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.73</td>\n",
       "      <td>8.0</td>\n",
       "      <td>6.89</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      Ix     Iy   IIx   IIy  IIIx   IIIy   IVx    IVy\n",
       "0   10.0   8.04  10.0  9.14  10.0   7.46   8.0   6.58\n",
       "1    8.0   6.95   8.0  8.14   8.0   6.77   8.0   5.76\n",
       "2   13.0   7.58  13.0  8.74  13.0  12.74   8.0   7.71\n",
       "3    9.0   8.81   9.0  8.77   9.0   7.11   8.0   8.84\n",
       "4   11.0   8.33  11.0  9.26  11.0   7.81   8.0   8.47\n",
       "5   14.0   9.96  14.0  8.10  14.0   8.84   8.0   7.04\n",
       "6    6.0   7.24   6.0  6.13   6.0   6.08   8.0   5.25\n",
       "7    4.0   4.26   4.0  3.10   4.0   5.39  19.0  12.50\n",
       "8   12.0  10.84  12.0  9.13  12.0   8.15   8.0   5.56\n",
       "9    7.0   4.82   7.0  7.26   7.0   6.42   8.0   7.91\n",
       "10   5.0   5.68   5.0  4.74   5.0   5.73   8.0   6.89"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_columns=[\n",
    "[10.0,   8.04,   10.0,   9.14,   10.0,   7.46,   8.0,    6.58],\n",
    "[8.0,    6.95,   8.0,    8.14,   8.0,    6.77,   8.0,    5.76],\n",
    "[13.0,   7.58,   13.0,   8.74,   13.0,   12.74,  8.0,    7.71],\n",
    "[9.0,    8.81,   9.0,    8.77,   9.0,    7.11,   8.0,    8.84],\n",
    "[11.0,   8.33,   11.0,   9.26,   11.0,   7.81,   8.0,    8.47],\n",
    "[14.0,   9.96,   14.0,   8.10,   14.0,   8.84,   8.0,    7.04],\n",
    "[6.0,    7.24,   6.0,    6.13,   6.0,    6.08,   8.0,    5.25],\n",
    "[4.0,    4.26,   4.0,    3.10,   4.0,    5.39,   19.0,   12.5],\n",
    "[12.0,   10.84,  12.0,   9.13,   12.0,   8.15,   8.0,    5.56],\n",
    "[7.0,    4.82,   7.0,    7.26,   7.0,    6.42,   8.0,    7.91],\n",
    "[5.0,    5.68,   5.0,    4.74,   5.0,    5.73,   8.0,    6.89]]\n",
    "\n",
    "n = [['I', 'II', 'III', 'IV'], ['x', 'y']]\n",
    "indexes = [i+j for i in n[0] for j in n[1]]\n",
    "\n",
    "quartet = pd.DataFrame(data=raw_columns, columns=indexes)\n",
    "quartet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's list the available aliases to store our dataset. This is available under the `data` -> `resources` section in the yaml file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ".ascombe:\n",
      "  format: csv\n",
      "  path: datasets/ascombe.csv\n",
      "  provider: howto\n",
      ".corr:\n",
      "  format: csv\n",
      "  path: datasets/correlation.csv\n",
      "  provider: howto\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dlf.params.pretty_print(metadata['resources'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data provider `howto` is a local data file system."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "howto:\n",
      "  rootpath: data\n",
      "  service: fs\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dlf.params.pretty_print(metadata['providers'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's store the dataframe as csv, first using the engine help function, then directly using the pandas context. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# write using the engine utility\n",
    "engine.write(quartet, 'ascombe')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#write using the data.path utility\n",
    "quartet.to_csv(dlf.data.path('ascombe'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reading the data back to a DataFrame. Again first using the engine `read` utility, the directly using the pandas `read_csv` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>Ix</th>\n",
       "      <th>Iy</th>\n",
       "      <th>IIx</th>\n",
       "      <th>IIy</th>\n",
       "      <th>IIIx</th>\n",
       "      <th>IIIy</th>\n",
       "      <th>IVx</th>\n",
       "      <th>IVy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>8.04</td>\n",
       "      <td>10.0</td>\n",
       "      <td>9.14</td>\n",
       "      <td>10.0</td>\n",
       "      <td>7.46</td>\n",
       "      <td>8.0</td>\n",
       "      <td>6.58</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>8.0</td>\n",
       "      <td>6.95</td>\n",
       "      <td>8.0</td>\n",
       "      <td>8.14</td>\n",
       "      <td>8.0</td>\n",
       "      <td>6.77</td>\n",
       "      <td>8.0</td>\n",
       "      <td>5.76</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>13.0</td>\n",
       "      <td>7.58</td>\n",
       "      <td>13.0</td>\n",
       "      <td>8.74</td>\n",
       "      <td>13.0</td>\n",
       "      <td>12.74</td>\n",
       "      <td>8.0</td>\n",
       "      <td>7.71</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>9.0</td>\n",
       "      <td>8.81</td>\n",
       "      <td>9.0</td>\n",
       "      <td>8.77</td>\n",
       "      <td>9.0</td>\n",
       "      <td>7.11</td>\n",
       "      <td>8.0</td>\n",
       "      <td>8.84</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>11.0</td>\n",
       "      <td>8.33</td>\n",
       "      <td>11.0</td>\n",
       "      <td>9.26</td>\n",
       "      <td>11.0</td>\n",
       "      <td>7.81</td>\n",
       "      <td>8.0</td>\n",
       "      <td>8.47</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>5</td>\n",
       "      <td>14.0</td>\n",
       "      <td>9.96</td>\n",
       "      <td>14.0</td>\n",
       "      <td>8.10</td>\n",
       "      <td>14.0</td>\n",
       "      <td>8.84</td>\n",
       "      <td>8.0</td>\n",
       "      <td>7.04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>6</td>\n",
       "      <td>6.0</td>\n",
       "      <td>7.24</td>\n",
       "      <td>6.0</td>\n",
       "      <td>6.13</td>\n",
       "      <td>6.0</td>\n",
       "      <td>6.08</td>\n",
       "      <td>8.0</td>\n",
       "      <td>5.25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>7</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.26</td>\n",
       "      <td>4.0</td>\n",
       "      <td>3.10</td>\n",
       "      <td>4.0</td>\n",
       "      <td>5.39</td>\n",
       "      <td>19.0</td>\n",
       "      <td>12.50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>8</td>\n",
       "      <td>12.0</td>\n",
       "      <td>10.84</td>\n",
       "      <td>12.0</td>\n",
       "      <td>9.13</td>\n",
       "      <td>12.0</td>\n",
       "      <td>8.15</td>\n",
       "      <td>8.0</td>\n",
       "      <td>5.56</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>9</td>\n",
       "      <td>7.0</td>\n",
       "      <td>4.82</td>\n",
       "      <td>7.0</td>\n",
       "      <td>7.26</td>\n",
       "      <td>7.0</td>\n",
       "      <td>6.42</td>\n",
       "      <td>8.0</td>\n",
       "      <td>7.91</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>10</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.68</td>\n",
       "      <td>5.0</td>\n",
       "      <td>4.74</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.73</td>\n",
       "      <td>8.0</td>\n",
       "      <td>6.89</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    Unnamed: 0    Ix     Iy   IIx   IIy  IIIx   IIIy   IVx    IVy\n",
       "0            0  10.0   8.04  10.0  9.14  10.0   7.46   8.0   6.58\n",
       "1            1   8.0   6.95   8.0  8.14   8.0   6.77   8.0   5.76\n",
       "2            2  13.0   7.58  13.0  8.74  13.0  12.74   8.0   7.71\n",
       "3            3   9.0   8.81   9.0  8.77   9.0   7.11   8.0   8.84\n",
       "4            4  11.0   8.33  11.0  9.26  11.0   7.81   8.0   8.47\n",
       "5            5  14.0   9.96  14.0  8.10  14.0   8.84   8.0   7.04\n",
       "6            6   6.0   7.24   6.0  6.13   6.0   6.08   8.0   5.25\n",
       "7            7   4.0   4.26   4.0  3.10   4.0   5.39  19.0  12.50\n",
       "8            8  12.0  10.84  12.0  9.13  12.0   8.15   8.0   5.56\n",
       "9            9   7.0   4.82   7.0  7.26   7.0   6.42   8.0   7.91\n",
       "10          10   5.0   5.68   5.0  4.74   5.0   5.73   8.0   6.89"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#read using the engine utility\n",
    "engine.read('ascombe')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read using the context\n",
    "pd = engine.context()\n",
    "quartet = pd.read_csv(dlf.data.path('.ascombe'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "for s in ['I', 'II', 'III', 'IV']:\n",
    "    ax.scatter(quartet[s+'x'],quartet[s+'y'], label=s)\n",
    "ax.set_title('ascombe sets')\n",
    "ax.legend()\n",
    "plt;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Context: Spark__  \n",
    "Let's start the engine session, by selecting a spark context from the list. Your can have many spark contexts declared, for instance for single node "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "engine = dlf.engines.get('howto_spark')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can quickly inspect the properties of the context by calling the `info()` function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'name': 'howto_spark',\n",
       " 'context': 'spark',\n",
       " 'config': {'master': 'local[*]', 'jobname': 'howto'}}"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "engine.info"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By calling the `context` method, you access the Spark SQL Context directly. The rest of your spark python code is not affected by the initialization of your session with the datalabframework."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'spark:2.3.0'"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark = engine.context()\n",
    "\n",
    "#print out name and version\n",
    "'{}:{}'.format(engine.info['context'], spark.sparkSession.version)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once again, let's read the csv data again, this time using the spark context. First using the engine `write` utility, then directly using the spark context and the `dlf.data.path` function to localize our labeled dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "#read using the engine utility\n",
    "df = engine.read('.ascombe')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "#read using the spark dataframe method\n",
    "df = spark.read.csv(dlf.data.path('.ascombe'), inferSchema=True, header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- _c0: integer (nullable = true)\n",
      " |-- Ix: double (nullable = true)\n",
      " |-- Iy: double (nullable = true)\n",
      " |-- IIx: double (nullable = true)\n",
      " |-- IIy: double (nullable = true)\n",
      " |-- IIIx: double (nullable = true)\n",
      " |-- IIIy: double (nullable = true)\n",
      " |-- IVx: double (nullable = true)\n",
      " |-- IVy: double (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----+-----+----+----+----+-----+----+----+\n",
      "|_c0|  Ix|   Iy| IIx| IIy|IIIx| IIIy| IVx| IVy|\n",
      "+---+----+-----+----+----+----+-----+----+----+\n",
      "|  0|10.0| 8.04|10.0|9.14|10.0| 7.46| 8.0|6.58|\n",
      "|  1| 8.0| 6.95| 8.0|8.14| 8.0| 6.77| 8.0|5.76|\n",
      "|  2|13.0| 7.58|13.0|8.74|13.0|12.74| 8.0|7.71|\n",
      "|  3| 9.0| 8.81| 9.0|8.77| 9.0| 7.11| 8.0|8.84|\n",
      "|  4|11.0| 8.33|11.0|9.26|11.0| 7.81| 8.0|8.47|\n",
      "|  5|14.0| 9.96|14.0| 8.1|14.0| 8.84| 8.0|7.04|\n",
      "|  6| 6.0| 7.24| 6.0|6.13| 6.0| 6.08| 8.0|5.25|\n",
      "|  7| 4.0| 4.26| 4.0| 3.1| 4.0| 5.39|19.0|12.5|\n",
      "|  8|12.0|10.84|12.0|9.13|12.0| 8.15| 8.0|5.56|\n",
      "|  9| 7.0| 4.82| 7.0|7.26| 7.0| 6.42| 8.0|7.91|\n",
      "| 10| 5.0| 5.68| 5.0|4.74| 5.0| 5.73| 8.0|6.89|\n",
      "+---+----+-----+----+----+----+-----+----+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, let's calculate the correlation for each set I,II, III, IV between the `x` and `y` columns and save the result on an separate dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------------+-----------+------------+-----------+\n",
      "|_c0|           I|         II|         III|         IV|\n",
      "+---+------------+-----------+------------+-----------+\n",
      "|  0| [10.0,8.04]|[10.0,9.14]| [10.0,7.46]| [8.0,6.58]|\n",
      "|  1|  [8.0,6.95]| [8.0,8.14]|  [8.0,6.77]| [8.0,5.76]|\n",
      "|  2| [13.0,7.58]|[13.0,8.74]|[13.0,12.74]| [8.0,7.71]|\n",
      "|  3|  [9.0,8.81]| [9.0,8.77]|  [9.0,7.11]| [8.0,8.84]|\n",
      "|  4| [11.0,8.33]|[11.0,9.26]| [11.0,7.81]| [8.0,8.47]|\n",
      "|  5| [14.0,9.96]| [14.0,8.1]| [14.0,8.84]| [8.0,7.04]|\n",
      "|  6|  [6.0,7.24]| [6.0,6.13]|  [6.0,6.08]| [8.0,5.25]|\n",
      "|  7|  [4.0,4.26]|  [4.0,3.1]|  [4.0,5.39]|[19.0,12.5]|\n",
      "|  8|[12.0,10.84]|[12.0,9.13]| [12.0,8.15]| [8.0,5.56]|\n",
      "|  9|  [7.0,4.82]| [7.0,7.26]|  [7.0,6.42]| [8.0,7.91]|\n",
      "| 10|  [5.0,5.68]| [5.0,4.74]|  [5.0,5.73]| [8.0,6.89]|\n",
      "+---+------------+-----------+------------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.feature import VectorAssembler\n",
    "\n",
    "for s in ['I', 'II', 'III', 'IV']:\n",
    "    va = VectorAssembler(inputCols=[s+'x', s+'y'], outputCol=s)\n",
    "    df = va.transform(df)\n",
    "    df = df.drop(s+'x', s+'y')\n",
    "    \n",
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After assembling the dataframe into four sets of 2D vectors, let's calculate the pearson correlation for each set. In the case the the ascombe sets, all sets should have the same pearson correlation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.stat import Correlation\n",
    "from pyspark.sql.types import DoubleType\n",
    "\n",
    "corr = {}\n",
    "cols = ['I', 'II', 'III', 'IV']\n",
    "\n",
    "# calculate pearson correlations\n",
    "for s in cols:\n",
    "    corr[s] = Correlation.corr(df, s, 'pearson').collect()[0][0][0,1].item()\n",
    "\n",
    "# declare schema\n",
    "from pyspark.sql.types import StructType, StructField, FloatType\n",
    "schema = StructType([StructField(s, FloatType(), True) for s in cols])\n",
    "\n",
    "# create output dataframe\n",
    "corr_df = spark.createDataFrame(data=[corr], schema=schema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-----+-----+-----+\n",
      "|    I|   II|  III|   IV|\n",
      "+-----+-----+-----+-----+\n",
      "|0.816|0.816|0.816|0.817|\n",
      "+-----+-----+-----+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pyspark.sql.functions as f\n",
    "corr_df.select([f.round(f.avg(c), 3).alias(c) for c in cols]).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save the results. It's a very small data frame, however Spark when saving  csv format files, assumes large data sets and partitions the files inside an object (a directory) with the name of the target file. See below:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "engine.write(corr_df,'corr', mode='overwrite')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We read it back to chack all went fine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+---------+----------+----------+\n",
      "|      _c0|      _c1|       _c2|       _c3|\n",
      "+---------+---------+----------+----------+\n",
      "|0.8164205|0.8162365|0.81628674|0.81652147|\n",
      "+---------+---------+----------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "engine.read('corr').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Modules: Export\n",
    "\n",
    "This submodules will allow you to export cells and import them in other notebooks as python packages. Check the notebook [hello.ipynb](hello.ipynb), where you will see how to export the notebook, then follow the code here below to check it really works!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "importing Jupyter notebook from hello.ipynb\n"
     ]
    }
   ],
   "source": [
    "from hello import hi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hi World!\n"
     ]
    }
   ],
   "source": [
    "hi()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
