{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Titanic Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import datalabframework as dlf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/jovyan/src'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dlf.project.rootpath()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['/home/jovyan/src/metadata.yml', '/home/jovyan/src/etl/raw/metadata.yml', '/home/jovyan/src/etl/extract/metadata.yml', '/home/jovyan/src/etl/clean/metadata.yml', '/home/jovyan/src/ml/features/metadata.yml']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'engines': {'spark': {'context': 'spark',\n",
       "   'config': {'master': 'local[4]', 'jobname': 'test'}},\n",
       "  'pandas': {'context': 'pandas'},\n",
       "  'numpy': {'context': 'numpy'}},\n",
       " 'loggers': {'stream': {'enable': True, 'severity': 'info'},\n",
       "  'kafka': {'enable': False,\n",
       "   'severity': 'info',\n",
       "   'topic': 'dlf',\n",
       "   'hosts': ['localhost:29092']}},\n",
       " 'providers': {'local': {'service': 'fs', 'rootpath': '../data'}},\n",
       " 'resources': {'.etl.extract.train': {'path': 'datasets/extract/train',\n",
       "   'format': 'parquet',\n",
       "   'provider': 'local'},\n",
       "  '.etl.clean.train': {'path': 'datasets/clean/train',\n",
       "   'format': 'parquet',\n",
       "   'provider': 'local'},\n",
       "  '.ml.features.test': {'path': 'datasets/features/test',\n",
       "   'format': 'parquet',\n",
       "   'provider': 'local'},\n",
       "  '.ml.features.train': {'path': 'datasets/features/train',\n",
       "   'format': 'parquet',\n",
       "   'provider': 'local'},\n",
       "  '.etl.raw.test': {'path': 'datasets/raw/test.csv',\n",
       "   'format': 'csv',\n",
       "   'provider': 'local'},\n",
       "  '.etl.clean.test': {'path': 'datasets/clean/test',\n",
       "   'format': 'parquet',\n",
       "   'provider': 'local'},\n",
       "  '.etl.extract.test': {'path': 'datasets/extract/test',\n",
       "   'format': 'parquet',\n",
       "   'provider': 'local'},\n",
       "  '.etl.raw.train': {'path': 'datasets/raw/train.csv',\n",
       "   'format': 'csv',\n",
       "   'provider': 'local'}}}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metadata = dlf.params.metadata()\n",
    "metadata"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "#### Init Spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['/home/jovyan/src/metadata.yml', '/home/jovyan/src/etl/raw/metadata.yml', '/home/jovyan/src/etl/extract/metadata.yml', '/home/jovyan/src/etl/clean/metadata.yml', '/home/jovyan/src/ml/features/metadata.yml']\n"
     ]
    }
   ],
   "source": [
    "engine = dlf.engines.get('spark')\n",
    "spark = engine.context()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'spark:2.3.0'"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#print out name and version\n",
    "'{}:{}'.format(engine.info['context'], spark.sparkSession.version)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "#### Load Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PassengerId  Integer    True\n",
      "Survived     Integer    True\n",
      "Pclass       Integer    True\n",
      "Name         String     True\n",
      "Sex          String     True\n",
      "Age          Double     True\n",
      "SibSp        Integer    True\n",
      "Parch        Integer    True\n",
      "Ticket       String     True\n",
      "Fare         Double     True\n",
      "Cabin        String     True\n",
      "Embarked     String     True\n"
     ]
    }
   ],
   "source": [
    "df = engine.read('.etl.raw.train', header=True, inferSchema=True)\n",
    "for column in df.schema:\n",
    "    print('{:<12} {:<10} {}'.format(column.name, str(column.dataType)[:-4], column.nullable))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "#### Save as parquet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "engine.write(df, '.etl.extract.train', mode='overwrite')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "#### Explore the dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check for Null or NaN values, and count them per column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+--------+------+----+---+---+-----+-----+------+----+-----+--------+\n",
      "|PassengerId|Survived|Pclass|Name|Sex|Age|SibSp|Parch|Ticket|Fare|Cabin|Embarked|\n",
      "+-----------+--------+------+----+---+---+-----+-----+------+----+-----+--------+\n",
      "|          0|       0|     0|   0|  0|177|    0|    0|     0|   0|  687|       2|\n",
      "+-----------+--------+------+----+---+---+-----+-----+------+----+-----+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import isnan, when, count, col\n",
    "df.select([count(when(isnan(c) | col(c).isNull(), c)).alias(c) for c in df.columns]).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "#### Load Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PassengerId  Integer    True\n",
      "Pclass       Integer    True\n",
      "Name         String     True\n",
      "Sex          String     True\n",
      "Age          Double     True\n",
      "SibSp        Integer    True\n",
      "Parch        Integer    True\n",
      "Ticket       String     True\n",
      "Fare         Double     True\n",
      "Cabin        String     True\n",
      "Embarked     String     True\n"
     ]
    }
   ],
   "source": [
    "df = engine.read('.etl.raw.test', header=True, inferSchema=True)\n",
    "for column in df.schema:\n",
    "    print('{:<12} {:<10} {}'.format(column.name, str(column.dataType)[:-4], column.nullable))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "#### Save as parquet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "engine.write(df,'.etl.extract.test', mode='overwrite')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "#### Explore the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PassengerId  Integer    True\n",
      "Pclass       Integer    True\n",
      "Name         String     True\n",
      "Sex          String     True\n",
      "Age          Double     True\n",
      "SibSp        Integer    True\n",
      "Parch        Integer    True\n",
      "Ticket       String     True\n",
      "Fare         Double     True\n",
      "Cabin        String     True\n",
      "Embarked     String     True\n"
     ]
    }
   ],
   "source": [
    "for column in df.schema:\n",
    "    print('{:<12} {:<10} {}'.format(column.name, str(column.dataType)[:-4], column.nullable))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check for Null or NaN values, and count them per column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+------+----+---+---+-----+-----+------+----+-----+--------+\n",
      "|PassengerId|Pclass|Name|Sex|Age|SibSp|Parch|Ticket|Fare|Cabin|Embarked|\n",
      "+-----------+------+----+---+---+-----+-----+------+----+-----+--------+\n",
      "|          0|     0|   0|  0| 86|    0|    0|     0|   1|  327|       0|\n",
      "+-----------+------+----+---+---+-----+-----+------+----+-----+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import isnan, when, count, col\n",
    "df.select([count(when(isnan(c) | col(c).isNull(), c)).alias(c) for c in df.columns]).show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
