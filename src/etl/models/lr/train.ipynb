{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datalabframework as dlf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/natbusa/Projects/dsp-titanic/src'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dlf.project.rootpath()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Init Spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "engine = dlf.engines.get('spark')\n",
    "spark = engine.context()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'spark:2.3.1'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#print out name and version\n",
    "'{}:{}'.format(engine.info['context'], spark.sparkSession.version)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = engine.read('.etl.features.train')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----+----------------------------------------------------------------+\n",
      "|id |label|features                                                        |\n",
      "+---+-----+----------------------------------------------------------------+\n",
      "|1  |0    |(19,[0,1,2,5,10,16,17],[22.0,7.25,1.0,1.0,1.0,1.0,1.0])         |\n",
      "|2  |1    |(19,[0,1,3,5,10,18],[38.0,71.2833,1.0,1.0,1.0,1.0])             |\n",
      "|3  |1    |(19,[0,1,2,4,10,17],[26.0,7.925,1.0,1.0,1.0,1.0])               |\n",
      "|4  |1    |(19,[0,1,3,5,10,17],[35.0,53.1,1.0,1.0,1.0,1.0])                |\n",
      "|5  |0    |(19,[0,1,2,4,10,16,17],[35.0,8.05,1.0,1.0,1.0,1.0,1.0])         |\n",
      "|6  |0    |(19,[0,1,2,4,10,16],[28.963899873494455,8.4583,1.0,1.0,1.0,1.0])|\n",
      "|7  |0    |(19,[0,1,3,4,10,16,17],[54.0,51.8625,1.0,1.0,1.0,1.0,1.0])      |\n",
      "|8  |0    |(19,[0,1,2,8,11,16,17],[2.0,21.075,1.0,1.0,1.0,1.0,1.0])        |\n",
      "|9  |1    |(19,[0,1,2,4,12,17],[27.0,11.1333,1.0,1.0,1.0,1.0])             |\n",
      "|10 |1    |(19,[0,1,5,10,18],[14.0,30.0708,1.0,1.0,1.0])                   |\n",
      "|11 |1    |(19,[0,1,2,5,11,17],[4.0,16.7,1.0,1.0,1.0,1.0])                 |\n",
      "|12 |1    |(19,[0,1,3,4,10,17],[58.0,26.55,1.0,1.0,1.0,1.0])               |\n",
      "|13 |0    |(19,[0,1,2,4,10,16,17],[20.0,8.05,1.0,1.0,1.0,1.0,1.0])         |\n",
      "|14 |0    |(19,[0,1,2,5,14,16,17],[39.0,31.275,1.0,1.0,1.0,1.0,1.0])       |\n",
      "|15 |0    |(19,[0,1,2,4,10,17],[14.0,7.8542,1.0,1.0,1.0,1.0])              |\n",
      "|16 |1    |(19,[0,1,4,10,17],[55.0,16.0,1.0,1.0,1.0])                      |\n",
      "|17 |0    |(19,[0,1,2,7,11,16],[2.0,29.125,1.0,1.0,1.0,1.0])               |\n",
      "|18 |1    |(19,[0,1,4,10,16,17],[33.00598190211949,13.0,1.0,1.0,1.0,1.0])  |\n",
      "|19 |0    |(19,[0,1,2,5,10,17],[31.0,18.0,1.0,1.0,1.0,1.0])                |\n",
      "|20 |1    |(19,[0,1,2,4,10,18],[25.220669460419217,7.225,1.0,1.0,1.0,1.0]) |\n",
      "+---+-----+----------------------------------------------------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.classification import LogisticRegression\n",
    "\n",
    "lr = LogisticRegression()\n",
    "pipeline = Pipeline(stages=[lr])\n",
    "\n",
    "model = pipeline.fit(df)\n",
    "prediction = model.transform(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+----------+-----------------------------------------+\n",
      "|label|prediction|probability                              |\n",
      "+-----+----------+-----------------------------------------+\n",
      "|0    |0.0       |[0.8851298504855403,0.11487014951445967] |\n",
      "|1    |1.0       |[0.0689883852034645,0.9310116147965354]  |\n",
      "|1    |1.0       |[0.39961202844990046,0.6003879715500996] |\n",
      "|1    |1.0       |[0.08200469956317774,0.9179953004368222] |\n",
      "|0    |0.0       |[0.9318137062197771,0.06818629378022292] |\n",
      "|0    |0.0       |[0.8826418619093687,0.11735813809063134] |\n",
      "|0    |0.0       |[0.7436267474390812,0.25637325256091886] |\n",
      "|0    |0.0       |[0.9632642722416761,0.036735727758323775]|\n",
      "|1    |1.0       |[0.41128199780062663,0.5887180021993734] |\n",
      "|1    |1.0       |[0.08626684320344105,0.913733156796559]  |\n",
      "|1    |1.0       |[0.15456883133788948,0.8454311686621105] |\n",
      "|1    |1.0       |[0.20025482614849038,0.7997451738515096] |\n",
      "|0    |0.0       |[0.8826901205527755,0.11730987944722451] |\n",
      "|0    |0.0       |[0.9791697215747085,0.02083027842529163] |\n",
      "|0    |1.0       |[0.2922845980122724,0.7077154019877276]  |\n",
      "|1    |1.0       |[0.4075201783865964,0.5924798216134036]  |\n",
      "|0    |0.0       |[0.9028985645941762,0.09710143540582383] |\n",
      "|1    |0.0       |[0.8055860387331142,0.1944139612668859]  |\n",
      "|0    |1.0       |[0.42854520495280435,0.5714547950471957] |\n",
      "|1    |1.0       |[0.33115397979486694,0.6688460202051331] |\n",
      "+-----+----------+-----------------------------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "prediction.select('label', 'prediction','probability').show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation areaUnderROC : 0.8636356373629883\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
    "\n",
    "evaluator = BinaryClassificationEvaluator()\n",
    "\n",
    "metric = evaluator.evaluate(prediction)\n",
    "metric_name = evaluator.getMetricName()\n",
    "\n",
    "print(\"Evaluation {} : {}\".format(metric_name, metric))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.write().overwrite().save(path='lr2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml import PipelineModel\n",
    "m2 = PipelineModel.load(path='lr2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation areaUnderROC : 0.8636356373629883\n"
     ]
    }
   ],
   "source": [
    "prediction = m2.transform(df)\n",
    "metric = evaluator.evaluate(prediction)\n",
    "metric_name = evaluator.getMetricName()\n",
    "\n",
    "print(\"Evaluation {} : {}\".format(metric_name, metric))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
