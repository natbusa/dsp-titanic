{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datalabframework as dlf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "engine = dlf.engines.get('spark')\n",
    "spark = engine.context()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'spark:2.3.1'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#print out name and version\n",
    "'{}:{}'.format(engine.info['context'], spark.sparkSession.version)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df={}\n",
    "for t in ['train', 'test']:\n",
    "    df[t] = engine.read('.etl.extract.{}'.format(t))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+--------+------+----+---+---+-----+-----+------+----+-----+--------+\n",
      "|PassengerId|Survived|Pclass|Name|Sex|Age|SibSp|Parch|Ticket|Fare|Cabin|Embarked|\n",
      "+-----------+--------+------+----+---+---+-----+-----+------+----+-----+--------+\n",
      "|          0|       0|     0|   0|  0|177|    0|    0|     0|   0|  687|       2|\n",
      "+-----------+--------+------+----+---+---+-----+-----+------+----+-----+--------+\n",
      "\n",
      "+-----------+------+----+---+---+-----+-----+------+----+-----+--------+\n",
      "|PassengerId|Pclass|Name|Sex|Age|SibSp|Parch|Ticket|Fare|Cabin|Embarked|\n",
      "+-----------+------+----+---+---+-----+-----+------+----+-----+--------+\n",
      "|          0|     0|   0|  0| 86|    0|    0|     0|   1|  327|       0|\n",
      "+-----------+------+----+---+---+-----+-----+------+----+-----+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import isnan, when, count, col\n",
    "\n",
    "for t in ['train', 'test']:\n",
    "    df[t].select([count(when(isnan(c) | col(c).isNull(), c)).alias(c) for c in df[t].columns]).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dropped columns\n",
    "dropped_columns = ['Ticket', 'Cabin']\n",
    "\n",
    "for t in ['train', 'test']:\n",
    "    df[t] = df[t].drop(*dropped_columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Embarked</th>\n",
       "      <th>count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Q</td>\n",
       "      <td>77</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>None</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>C</td>\n",
       "      <td>168</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>S</td>\n",
       "      <td>644</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Embarked  count\n",
       "0        Q     77\n",
       "1     None      2\n",
       "2        C    168\n",
       "3        S    644"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['train'].groupBy('Embarked').count().toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#simple fill for Fare, Price, Embarked\n",
    "def fill_with_mode(df, colname):\n",
    "    # which value is occuring most often?\n",
    "    d = df.groupBy(colname).count().toPandas()\n",
    "    fill_value = d.loc[d['count'].idxmax,colname]\n",
    "    print('Filling column {} with value: {}'.format(colname, fill_value))\n",
    "\n",
    "    #fill the na\n",
    "    df = df.fillna(fill_value, colname)\n",
    "    return df\n",
    "\n",
    "\n",
    "from pyspark.sql.functions import avg\n",
    "def fill_with_mean(df, colname):\n",
    "    # which is the average / mean value?\n",
    "    d = df.select(avg(colname)).collect()\n",
    "    fill_value = d[0][0]\n",
    "    print('Filling column {} with value: {}'.format(colname, fill_value))\n",
    "    \n",
    "    #fill the na\n",
    "    df = df.fillna(fill_value, colname)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-- train -----\n",
      "Filling column Embarked with value: S\n",
      "Filling column Fare with value: 32.2042079685746\n",
      "-- test -----\n",
      "Filling column Embarked with value: S\n",
      "Filling column Fare with value: 35.6271884892086\n"
     ]
    }
   ],
   "source": [
    "for t in ['train', 'test']:\n",
    "    print('-- {} -----'.format(t))\n",
    "    df[t] = fill_with_mode(df[t], 'Embarked')    \n",
    "    df[t] = fill_with_mean(df[t], 'Fare')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filling column Age with value: 29.69911764705882\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DataFrame[PassengerId: int, Survived: int, Pclass: int, Name: string, Sex: string, Age: double, SibSp: int, Parch: int, Fare: double, Embarked: string]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fill_with_mean(df['train'], 'Age')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+--------+------+----+---+---+-----+-----+----+--------+\n",
      "|PassengerId|Survived|Pclass|Name|Sex|Age|SibSp|Parch|Fare|Embarked|\n",
      "+-----------+--------+------+----+---+---+-----+-----+----+--------+\n",
      "|          0|       0|     0|   0|  0|177|    0|    0|   0|       0|\n",
      "+-----------+--------+------+----+---+---+-----+-----+----+--------+\n",
      "\n",
      "+-----------+------+----+---+---+-----+-----+----+--------+\n",
      "|PassengerId|Pclass|Name|Sex|Age|SibSp|Parch|Fare|Embarked|\n",
      "+-----------+------+----+---+---+-----+-----+----+--------+\n",
      "|          0|     0|   0|  0| 86|    0|    0|   0|       0|\n",
      "+-----------+------+----+---+---+-----+-----+----+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import isnan, when, count, col\n",
    "\n",
    "for t in ['train', 'test']:\n",
    "    df[t].select([count(when(isnan(c) | col(c).isNull(), c)).alias(c) for c in df[t].columns]).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #EXPORT \n",
    "# from pyspark.ml import Pipeline\n",
    "# from pyspark.ml.feature import OneHotEncoder, VectorAssembler, StringIndexer\n",
    "# from pyspark.sql.functions import col\n",
    "\n",
    "# def string_indexer(features):\n",
    "#     for c in features:\n",
    "#         yield StringIndexer(inputCol=c, outputCol=c+'_I')\n",
    "\n",
    "# def onehot(features):\n",
    "#     #todo: reproducable mapping, \n",
    "#     #      here the mapping depends on the data provided\n",
    "#     for c in features:\n",
    "#         yield OneHotEncoder(inputCol=c, outputCol=c+'_C')\n",
    "\n",
    "# def vectorize(stringCols, numericDiscreteCols, numericContinuosCols):\n",
    "#     #todo: automatically classify columns\n",
    "#     reg_all_discrete_cols = numericDiscreteCols + [c+'_I' for c in stringCols]\n",
    "#     reg_all_cols = numericContinuosCols + [c+'_C' for c in reg_all_discrete_cols]\n",
    "\n",
    "#     print(reg_all_cols)\n",
    "\n",
    "#     si = list(string_indexer(stringCols))\n",
    "#     oh = list(onehot(reg_all_discrete_cols))\n",
    "#     ar = [VectorAssembler(inputCols=reg_all_cols, outputCol=\"features\")]\n",
    "    \n",
    "#     stages=si+oh+ar\n",
    "#     return stages\n",
    "\n",
    "# def featurize(df, idCol, labelCol, numericContinuosCols, numericDiscreteCols, stringCols):\n",
    "\n",
    "#     #set the pipeline\n",
    "#     stages = vectorize(stringCols, numericDiscreteCols, numericContinuosCols)\n",
    "#     pipeline = Pipeline(stages=stages)\n",
    "\n",
    "#     #fit\n",
    "#     model = pipeline.fit(df)\n",
    "\n",
    "#     #transform\n",
    "#     features_df = model.transform(df).select(col(idCol).alias('id'), col(labelCol).alias('label'),'features')\n",
    "    \n",
    "#     return features_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "importing Jupyter notebook from /home/natbusa/Projects/dsp-titanic/src/etl/features/features.ipynb\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import datalabframework as dlf\n",
    "\n",
    "dlf.project.rootpath()\n",
    "from etl.features.features import featurize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import RFormula\n",
    "from pyspark.ml.regression import LinearRegression\n",
    "from pyspark.sql.functions import coalesce\n",
    "\n",
    "from pyspark.sql.functions import udf\n",
    "from pyspark.sql.types import StringType\n",
    "\n",
    "def contains_na(df, columns):\n",
    "    d = df.select([count(when(isnan(c) | col(c).isNull(), c)).alias(c) for c in columns])\n",
    "    return sum(d.collect()[0])>0\n",
    "\n",
    "def learn_imputation(df):\n",
    "    # featurize\n",
    "    \n",
    "    # create lr estimator\n",
    "    lr = LinearRegression(maxIter=10, regParam=0.3, elasticNetParam=0.8)\n",
    "\n",
    "    # Fit the model, where label is not null\n",
    "    model = lr.fit(df.where(df.label.isNotNull()))\n",
    "    \n",
    "    # Summarize the model over the training set and print out some metrics\n",
    "    trainingSummary = model.summary\n",
    "    \n",
    "    print(\"Learning Linear Regression Model:\")\n",
    "    print(\" - RMSE: %f\" % trainingSummary.rootMeanSquaredError)\n",
    "    print(\" - r2: %f\" % trainingSummary.r2)\n",
    "    \n",
    "    return model\n",
    "\n",
    "def impute(df, df_features, model, idCol, labelCol):\n",
    "    # impute dependent variable\n",
    "    df_impute = model.transform(df_features)\n",
    "    \n",
    "    # join prediction with original dataframe\n",
    "    df = df.join(df_impute.select(col('id').alias(idCol),'prediction'), idCol, \"leftouter\") \n",
    "\n",
    "    # coalesce null using imputation\n",
    "    df =  df.withColumn(labelCol,coalesce(df[labelCol],df.prediction)).drop('prediction')\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Learning Linear Regression Model:\n",
      " - RMSE: 10.769170\n",
      " - r2: 0.334880\n",
      "-- train -----\n",
      "+-----------+--------+------+--------------------+------+------------------+-----+-----+-------+--------+\n",
      "|PassengerId|Survived|Pclass|                Name|   Sex|               Age|SibSp|Parch|   Fare|Embarked|\n",
      "+-----------+--------+------+--------------------+------+------------------+-----+-----+-------+--------+\n",
      "|        148|       0|     3|\"Ford, Miss. Robi...|female|               9.0|    2|    2| 34.375|       S|\n",
      "|        463|       0|     1|   Gee, Mr. Arthur H|  male|              47.0|    0|    0|   38.5|       S|\n",
      "|        471|       0|     3|   Keefe, Mr. Arthur|  male|27.844465003907715|    0|    0|   7.25|       S|\n",
      "|        496|       0|     3|Yousseff, Mr. Ger...|  male|26.552053955241124|    0|    0|14.4583|       C|\n",
      "|        833|       0|     3|      Saad, Mr. Amin|  male|26.552053955241124|    0|    0| 7.2292|       C|\n",
      "|        243|       0|     2|Coleridge, Mr. Re...|  male|              29.0|    0|    0|   10.5|       S|\n",
      "|        392|       1|     3|Jansson, Mr. Carl...|  male|              21.0|    0|    0| 7.7958|       S|\n",
      "|        540|       1|     1|Frolicher, Miss. ...|female|              22.0|    0|    2|   49.5|       C|\n",
      "|        623|       1|     3|    Nakid, Mr. Sahid|  male|              20.0|    1|    1|15.7417|       C|\n",
      "|        737|       0|     3|Ford, Mrs. Edward...|female|              48.0|    1|    3| 34.375|       S|\n",
      "|        858|       1|     1|Daly, Mr. Peter D...|  male|              51.0|    0|    0|  26.55|       S|\n",
      "|         31|       0|     1|Uruchurtu, Don. M...|  male|              40.0|    0|    0|27.7208|       C|\n",
      "|        516|       0|     1|Walker, Mr. Willi...|  male|              47.0|    0|    0|34.0208|       S|\n",
      "|         85|       1|     2| Ilett, Miss. Bertha|female|              17.0|    0|    0|   10.5|       S|\n",
      "|        137|       1|     1|Newsom, Miss. Hel...|female|              19.0|    0|    2|26.2833|       S|\n",
      "|        251|       0|     3|Reed, Mr. James G...|  male|27.844465003907715|    0|    0|   7.25|       S|\n",
      "|        451|       0|     2|West, Mr. Edwy Ar...|  male|              36.0|    1|    2|  27.75|       S|\n",
      "|        580|       1|     3| Jussila, Mr. Eiriik|  male|              32.0|    0|    0|  7.925|       S|\n",
      "|        808|       0|     3|Pettersson, Miss....|female|              18.0|    0|    0|  7.775|       S|\n",
      "|         65|       0|     1|Stewart, Mr. Albe...|  male|40.921522294320056|    0|    0|27.7208|       C|\n",
      "+-----------+--------+------+--------------------+------+------------------+-----+-----+-------+--------+\n",
      "only showing top 20 rows\n",
      "\n",
      "-- test -----\n",
      "+-----------+------+--------------------+------+------------------+-----+-----+-------+--------+\n",
      "|PassengerId|Pclass|                Name|   Sex|               Age|SibSp|Parch|   Fare|Embarked|\n",
      "+-----------+------+--------------------+------+------------------+-----+-----+-------+--------+\n",
      "|       1088|     1|Spedden, Master. ...|  male|               6.0|    0|    2|  134.5|       C|\n",
      "|       1238|     2|Botsford, Mr. Wil...|  male|              26.0|    0|    0|   13.0|       S|\n",
      "|        897|     3|Svensson, Mr. Joh...|  male|              14.0|    0|    0|  9.225|       S|\n",
      "|       1025|     3|Thomas, Mr. Charl...|  male|26.086106437810805|    1|    0| 6.4375|       C|\n",
      "|       1084|     3|van Billiard, Mas...|  male|              11.5|    1|    1|   14.5|       S|\n",
      "|       1127|     3|Vendel, Mr. Olof ...|  male|              20.0|    0|    0| 7.8542|       S|\n",
      "|       1139|     2|Drew, Mr. James V...|  male|              42.0|    1|    1|   32.5|       S|\n",
      "|       1143|     3|Abrahamsson, Mr. ...|  male|              20.0|    0|    0|  7.925|       S|\n",
      "|       1270|     1|Hipkins, Mr. Will...|  male|              55.0|    0|    0|   50.0|       S|\n",
      "|       1303|     1|Minahan, Mrs. Wil...|female|              37.0|    1|    0|   90.0|       Q|\n",
      "|       1265|     2|Harbeck, Mr. Will...|  male|              44.0|    0|    0|   13.0|       S|\n",
      "|       1223|     1|Dulles, Mr. Willi...|  male|              39.0|    0|    0|   29.7|       C|\n",
      "|        898|     3|Connolly, Miss. Kate|female|              30.0|    0|    0| 7.6292|       Q|\n",
      "|        970|     2|Aldworth, Mr. Cha...|  male|              30.0|    0|    0|   13.0|       S|\n",
      "|       1157|     3|Lyntakoff, Mr. St...|  male|27.844465003907715|    0|    0| 7.8958|       S|\n",
      "|        918|     1|Ostby, Miss. Hele...|female|              22.0|    0|    1|61.9792|       C|\n",
      "|       1005|     3|Buckley, Miss. Ka...|female|              18.5|    0|    0| 7.2833|       Q|\n",
      "|       1016|     3|   Kennedy, Mr. John|  male|27.844465003907715|    0|    0|   7.75|       Q|\n",
      "|       1133|     2|Christy, Mrs. (Al...|female|              45.0|    0|    2|   30.0|       S|\n",
      "|        961|     1|Fortune, Mrs. Mar...|female|              60.0|    1|    4|  263.0|       S|\n",
      "+-----------+------+--------------------+------+------------------+-----+-----+-------+--------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Learn from both trainn and test\n",
    "cols = set(df['train'].columns) & set(df['test'].columns)\n",
    "df_union = df['train'].select(*cols).union(df['test'].select(*cols))\n",
    "\n",
    "#select regressors and featurize\n",
    "pipeline = featurize(['Fare'], ['Pclass','SibSp','Parch'], ['Sex', 'Embarked'])\n",
    "model = pipeline.fit(df_union)\n",
    "\n",
    "# learn linear regression model\n",
    "d = model.transform(df_union).select(col('PassengerId').alias('id'), col('Age').alias('label'), 'features')\n",
    "impute_model = learn_imputation(d)\n",
    "\n",
    "for t in ['train', 'test']:\n",
    "    print('-- {} -----'.format(t))\n",
    "    df[t] = impute(df[t], d, impute_model, 'PassengerId', 'Age')\n",
    "    df[t].show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### No NA beyond this point"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "for t in ['train', 'test']:\n",
    "    assert not contains_na(df[t], df[t].columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Write the output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "for t in ['train', 'test']:\n",
    "    engine.write(df[t], '.etl.clean.{}'.format(t), mode='overwrite')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
