{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datalabframework as dlf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "engine = dlf.engines.get('spark')\n",
    "spark = engine.context()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'spark:2.3.0'"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#print out name and version\n",
    "'{}:{}'.format(engine.info['context'], spark.sparkSession.version)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "df={}\n",
    "for t in ['train', 'test']:\n",
    "    df[t] = engine.read('.etl.extract.{}'.format(t))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+--------+------+----+---+---+-----+-----+------+----+-----+--------+\n",
      "|PassengerId|Survived|Pclass|Name|Sex|Age|SibSp|Parch|Ticket|Fare|Cabin|Embarked|\n",
      "+-----------+--------+------+----+---+---+-----+-----+------+----+-----+--------+\n",
      "|          0|       0|     0|   0|  0|177|    0|    0|     0|   0|  687|       2|\n",
      "+-----------+--------+------+----+---+---+-----+-----+------+----+-----+--------+\n",
      "\n",
      "+-----------+------+----+---+---+-----+-----+------+----+-----+--------+\n",
      "|PassengerId|Pclass|Name|Sex|Age|SibSp|Parch|Ticket|Fare|Cabin|Embarked|\n",
      "+-----------+------+----+---+---+-----+-----+------+----+-----+--------+\n",
      "|          0|     0|   0|  0| 86|    0|    0|     0|   1|  327|       0|\n",
      "+-----------+------+----+---+---+-----+-----+------+----+-----+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import isnan, when, count, col\n",
    "\n",
    "for t in ['train', 'test']:\n",
    "    df[t].select([count(when(isnan(c) | col(c).isNull(), c)).alias(c) for c in df[t].columns]).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dropped columns\n",
    "dropped_columns = ['Ticket', 'Cabin']\n",
    "\n",
    "for t in ['train', 'test']:\n",
    "    df[t] = df[t].drop(*dropped_columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "#simple fill for Fare, Price, Embarked\n",
    "def fill_with_mode(df, colname):\n",
    "    # which value is occuring most often?\n",
    "    d = df.groupBy(colname).count().toPandas()\n",
    "    fill_value = d.loc[d['count'].idxmax,colname]\n",
    "    print('Filling column {} with value: {}'.format(colname, fill_value))\n",
    "\n",
    "    #fill the na\n",
    "    df = df.fillna(fill_value, colname)\n",
    "    return df\n",
    "\n",
    "\n",
    "from pyspark.sql.functions import avg\n",
    "def fill_with_mean(df, colname):\n",
    "    # which is the average / mean value?\n",
    "    d = df.select(avg(colname)).collect()\n",
    "    fill_value = d[0][0]\n",
    "    print('Filling column {} with value: {}'.format(colname, fill_value))\n",
    "    \n",
    "    #fill the na\n",
    "    df = df.fillna(fill_value, colname)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-- train -----\n",
      "Filling column Embarked with value: S\n",
      "Filling column Fare with value: 32.2042079685746\n",
      "-- test -----\n",
      "Filling column Embarked with value: S\n",
      "Filling column Fare with value: 35.6271884892086\n"
     ]
    }
   ],
   "source": [
    "for t in ['train', 'test']:\n",
    "    print('-- {} -----'.format(t))\n",
    "    df[t] = fill_with_mode(df[t], 'Embarked')    \n",
    "    df[t] = fill_with_mean(df[t], 'Fare')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+--------+------+----+---+---+-----+-----+----+--------+\n",
      "|PassengerId|Survived|Pclass|Name|Sex|Age|SibSp|Parch|Fare|Embarked|\n",
      "+-----------+--------+------+----+---+---+-----+-----+----+--------+\n",
      "|          0|       0|     0|   0|  0|177|    0|    0|   0|       0|\n",
      "+-----------+--------+------+----+---+---+-----+-----+----+--------+\n",
      "\n",
      "+-----------+------+----+---+---+-----+-----+----+--------+\n",
      "|PassengerId|Pclass|Name|Sex|Age|SibSp|Parch|Fare|Embarked|\n",
      "+-----------+------+----+---+---+-----+-----+----+--------+\n",
      "|          0|     0|   0|  0| 86|    0|    0|   0|       0|\n",
      "+-----------+------+----+---+---+-----+-----+----+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import isnan, when, count, col\n",
    "\n",
    "for t in ['train', 'test']:\n",
    "    df[t].select([count(when(isnan(c) | col(c).isNull(), c)).alias(c) for c in df[t].columns]).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import RFormula\n",
    "from pyspark.ml.regression import LinearRegression\n",
    "from pyspark.sql.functions import coalesce\n",
    "\n",
    "from pyspark.sql.functions import udf\n",
    "from pyspark.sql.types import StringType\n",
    "\n",
    "def contains_na(df, columns):\n",
    "    d = df.select([count(when(isnan(c) | col(c).isNull(), c)).alias(c) for c in columns])\n",
    "    return sum(d.collect()[0])>0\n",
    "\n",
    "def feature_vector(df, idcol, colname, regressors):\n",
    "    formula = RFormula(formula= colname + ' ~ ' + '+'.join(regressors), \n",
    "                   labelCol='label', featuresCol='features')\n",
    "    \n",
    "    # to dense feature vector\n",
    "    df_features = formula.fit(df).transform(df).select(idcol,'features', 'label')\n",
    "    \n",
    "    return df_features\n",
    "\n",
    "def impute(df, idcol, colname, regressors):\n",
    "    assert not contains_na(df, regressors)\n",
    "    \n",
    "    # to vector\n",
    "    df_features = feature_vector(df, idcol, colname, regressors)  \n",
    "    \n",
    "    # create lr estimator\n",
    "    lr = LinearRegression(maxIter=10, regParam=0.3, elasticNetParam=0.8)\n",
    "\n",
    "    # Fit the model\n",
    "    lrModel = lr.fit(df_features.where(df_features.label.isNotNull()))\n",
    "    \n",
    "    # Print the coefficients and intercept for linear regression\n",
    "    print(\"Coefficients: %s\" % str(lrModel.coefficients))\n",
    "    print(\"Intercept: %s\" % str(lrModel.intercept))\n",
    "\n",
    "    # Summarize the model over the training set and print out some metrics\n",
    "    trainingSummary = lrModel.summary\n",
    "\n",
    "    print(\"RMSE: %f\" % trainingSummary.rootMeanSquaredError)\n",
    "    print(\"r2: %f\" % trainingSummary.r2)\n",
    "    \n",
    "    # impute dependent variable\n",
    "    df_impute = lrModel.transform(df_features)\n",
    "    \n",
    "    # join prediction with original dataframe\n",
    "    df = df.join(df_impute.select('PassengerId','prediction'), 'PassengerId', \"leftouter\") \n",
    "    \n",
    "    # coalesce null using imputation\n",
    "    df =  df.withColumn(colname,coalesce(df[colname],df.prediction)).drop('prediction')\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-- train -----\n",
      "Coefficients: [0.0217424767527]\n",
      "Intercept: 28.944772982846892\n",
      "RMSE: 14.451254\n",
      "r2: 0.008945\n",
      "-- test -----\n",
      "Coefficients: [0.0734702047512]\n",
      "Intercept: 27.262813032167124\n",
      "RMSE: 13.344020\n",
      "r2: 0.111910\n"
     ]
    }
   ],
   "source": [
    "for t in ['train', 'test']:\n",
    "    print('-- {} -----'.format(t))\n",
    "    df[t] = impute_lr(df[t], 'PassengerId', 'Age', ['Fare']) \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### No NA beyond this point"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "for t in ['train', 'test']:\n",
    "    assert not contains_na(df[t], df[t].columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Write the output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "for t in ['train', 'test']:\n",
    "    engine.write(df[t], '.etl.clean.{}'.format(t), mode='overwrite')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
