{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Project rootpath: /home/jovyan/src\n"
     ]
    }
   ],
   "source": [
    "import datalabframework as dlf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "engine = dlf.engines.get('spark')\n",
    "spark = engine.context()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'spark:2.3.0'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#print out name and version\n",
    "'{}:{}'.format(engine.info['context'], spark.sparkSession.version)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df={}\n",
    "for t in ['train', 'test']:\n",
    "    df[t] = engine.read('.etl.extract.{}'.format(t))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+--------+------+----+---+---+-----+-----+------+----+-----+--------+\n",
      "|PassengerId|Survived|Pclass|Name|Sex|Age|SibSp|Parch|Ticket|Fare|Cabin|Embarked|\n",
      "+-----------+--------+------+----+---+---+-----+-----+------+----+-----+--------+\n",
      "|          0|       0|     0|   0|  0|177|    0|    0|     0|   0|  687|       2|\n",
      "+-----------+--------+------+----+---+---+-----+-----+------+----+-----+--------+\n",
      "\n",
      "+-----------+------+----+---+---+-----+-----+------+----+-----+--------+\n",
      "|PassengerId|Pclass|Name|Sex|Age|SibSp|Parch|Ticket|Fare|Cabin|Embarked|\n",
      "+-----------+------+----+---+---+-----+-----+------+----+-----+--------+\n",
      "|          0|     0|   0|  0| 86|    0|    0|     0|   1|  327|       0|\n",
      "+-----------+------+----+---+---+-----+-----+------+----+-----+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import isnan, when, count, col\n",
    "\n",
    "for t in ['train', 'test']:\n",
    "    df[t].select([count(when(isnan(c) | col(c).isNull(), c)).alias(c) for c in df[t].columns]).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dropped columns\n",
    "dropped_columns = ['Ticket', 'Cabin']\n",
    "\n",
    "for t in ['train', 'test']:\n",
    "    df[t] = df[t].drop(*dropped_columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#simple fill for Fare, Price, Embarked\n",
    "def fill_with_mode(df, colname):\n",
    "    # which value is occuring most often?\n",
    "    d = df.groupBy(colname).count().toPandas()\n",
    "    fill_value = d.loc[d['count'].idxmax,colname]\n",
    "    print('Filling column {} with value: {}'.format(colname, fill_value))\n",
    "\n",
    "    #fill the na\n",
    "    df = df.fillna(fill_value, colname)\n",
    "    return df\n",
    "\n",
    "\n",
    "from pyspark.sql.functions import avg\n",
    "def fill_with_mean(df, colname):\n",
    "    # which is the average / mean value?\n",
    "    d = df.select(avg(colname)).collect()\n",
    "    fill_value = d[0][0]\n",
    "    print('Filling column {} with value: {}'.format(colname, fill_value))\n",
    "    \n",
    "    #fill the na\n",
    "    df = df.fillna(fill_value, colname)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-- train -----\n",
      "Filling column Embarked with value: S\n",
      "Filling column Fare with value: 32.2042079685746\n",
      "-- test -----\n",
      "Filling column Embarked with value: S\n",
      "Filling column Fare with value: 35.6271884892086\n"
     ]
    }
   ],
   "source": [
    "for t in ['train', 'test']:\n",
    "    print('-- {} -----'.format(t))\n",
    "    df[t] = fill_with_mode(df[t], 'Embarked')    \n",
    "    df[t] = fill_with_mean(df[t], 'Fare')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+--------+------+----+---+---+-----+-----+----+--------+\n",
      "|PassengerId|Survived|Pclass|Name|Sex|Age|SibSp|Parch|Fare|Embarked|\n",
      "+-----------+--------+------+----+---+---+-----+-----+----+--------+\n",
      "|          0|       0|     0|   0|  0|177|    0|    0|   0|       0|\n",
      "+-----------+--------+------+----+---+---+-----+-----+----+--------+\n",
      "\n",
      "+-----------+------+----+---+---+-----+-----+----+--------+\n",
      "|PassengerId|Pclass|Name|Sex|Age|SibSp|Parch|Fare|Embarked|\n",
      "+-----------+------+----+---+---+-----+-----+----+--------+\n",
      "|          0|     0|   0|  0| 86|    0|    0|   0|       0|\n",
      "+-----------+------+----+---+---+-----+-----+----+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import isnan, when, count, col\n",
    "\n",
    "for t in ['train', 'test']:\n",
    "    df[t].select([count(when(isnan(c) | col(c).isNull(), c)).alias(c) for c in df[t].columns]).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#EXPORT \n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.feature import OneHotEncoder, VectorAssembler, StringIndexer\n",
    "from pyspark.sql.functions import col\n",
    "\n",
    "def string_indexer(features):\n",
    "    for c in features:\n",
    "        yield StringIndexer(inputCol=c, outputCol=c+'_I')\n",
    "\n",
    "def onehot(features):\n",
    "    #todo: reproducable mapping, \n",
    "    #      here the mapping depends on the data provided\n",
    "    for c in features:\n",
    "        yield OneHotEncoder(inputCol=c, outputCol=c+'_C')\n",
    "\n",
    "def vectorize(stringCols, numericDiscreteCols, numericContinuosCols):\n",
    "    #todo: automatically classify columns\n",
    "    reg_all_discrete_cols = numericDiscreteCols + [c+'_I' for c in stringCols]\n",
    "    reg_all_cols = numericContinuosCols + [c+'_C' for c in reg_all_discrete_cols]\n",
    "\n",
    "    print(reg_all_cols)\n",
    "\n",
    "    si = list(string_indexer(stringCols))\n",
    "    oh = list(onehot(reg_all_discrete_cols))\n",
    "    ar = [VectorAssembler(inputCols=reg_all_cols, outputCol=\"features\")]\n",
    "    \n",
    "    stages=si+oh+ar\n",
    "    return stages\n",
    "\n",
    "def featurize(df, idCol, labelCol, numericContinuosCols, numericDiscreteCols, stringCols):\n",
    "\n",
    "    #set the pipeline\n",
    "    stages = vectorize(stringCols, numericDiscreteCols, numericContinuosCols)\n",
    "    pipeline = Pipeline(stages=stages)\n",
    "\n",
    "    #fit\n",
    "    model = pipeline.fit(df)\n",
    "\n",
    "    #transform\n",
    "    features_df = model.transform(df).select(col(idCol).alias('id'), col(labelCol).alias('label'),'features')\n",
    "    \n",
    "    return features_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import RFormula\n",
    "from pyspark.ml.regression import LinearRegression\n",
    "from pyspark.sql.functions import coalesce\n",
    "\n",
    "from pyspark.sql.functions import udf\n",
    "from pyspark.sql.types import StringType\n",
    "\n",
    "def contains_na(df, columns):\n",
    "    d = df.select([count(when(isnan(c) | col(c).isNull(), c)).alias(c) for c in columns])\n",
    "    return sum(d.collect()[0])>0\n",
    "\n",
    "def impute(df, idCol, labelCol, numericContinuosCols, numericDiscreteCols, stringCols):\n",
    "    # featurize\n",
    "    df_features = featurize(df, idCol, labelCol, numericContinuosCols, numericDiscreteCols, stringCols)\n",
    "    \n",
    "    # create lr estimator\n",
    "    lr = LinearRegression(maxIter=10, regParam=0.3, elasticNetParam=0.8)\n",
    "\n",
    "    # Fit the model\n",
    "    lrModel = lr.fit(df_features.where(df_features.label.isNotNull()))\n",
    "    \n",
    "    # Print the coefficients and intercept for linear regression\n",
    "    #print(\"Coefficients:\")\n",
    "    #print(*(lrModel.coefficients), sep='\\n')\n",
    "    #print(\"Intercept: \\n{}\".format(lrModel.intercept))\n",
    "\n",
    "    # Summarize the model over the training set and print out some metrics\n",
    "    trainingSummary = lrModel.summary\n",
    "\n",
    "    print(\"RMSE: %f\" % trainingSummary.rootMeanSquaredError)\n",
    "    print(\"r2: %f\" % trainingSummary.r2)\n",
    "    \n",
    "    # impute dependent variable\n",
    "    df_impute = lrModel.transform(df_features)\n",
    "    \n",
    "    # join prediction with original dataframe\n",
    "    df = df.join(df_impute.select(col('id').alias(idCol),'prediction'), idCol, \"leftouter\") \n",
    "\n",
    "    # coalesce null using imputation\n",
    "    df =  df.withColumn(labelCol,coalesce(df[labelCol],df.prediction)).drop('prediction')\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-- train -----\n",
      "['Fare', 'Pclass_C', 'SibSp_C', 'Parch_C', 'Sex_I_C', 'Embarked_I_C']\n",
      "RMSE: 12.072084\n",
      "r2: 0.308406\n",
      "+-----------+--------+------+--------------------+------+------------------+-----+-----+-------+--------+\n",
      "|PassengerId|Survived|Pclass|                Name|   Sex|               Age|SibSp|Parch|   Fare|Embarked|\n",
      "+-----------+--------+------+--------------------+------+------------------+-----+-----+-------+--------+\n",
      "|          1|       0|     3|Braund, Mr. Owen ...|  male|              22.0|    1|    0|   7.25|       S|\n",
      "|          2|       1|     1|Cumings, Mrs. Joh...|female|              38.0|    1|    0|71.2833|       C|\n",
      "|          3|       1|     3|Heikkinen, Miss. ...|female|              26.0|    0|    0|  7.925|       S|\n",
      "|          4|       1|     1|Futrelle, Mrs. Ja...|female|              35.0|    1|    0|   53.1|       S|\n",
      "|          5|       0|     3|Allen, Mr. Willia...|  male|              35.0|    0|    0|   8.05|       S|\n",
      "|          6|       0|     3|    Moran, Mr. James|  male|28.963899873494455|    0|    0| 8.4583|       Q|\n",
      "|          7|       0|     1|McCarthy, Mr. Tim...|  male|              54.0|    0|    0|51.8625|       S|\n",
      "|          8|       0|     3|Palsson, Master. ...|  male|               2.0|    3|    1| 21.075|       S|\n",
      "|          9|       1|     3|Johnson, Mrs. Osc...|female|              27.0|    0|    2|11.1333|       S|\n",
      "|         10|       1|     2|Nasser, Mrs. Nich...|female|              14.0|    1|    0|30.0708|       C|\n",
      "|         11|       1|     3|Sandstrom, Miss. ...|female|               4.0|    1|    1|   16.7|       S|\n",
      "|         12|       1|     1|Bonnell, Miss. El...|female|              58.0|    0|    0|  26.55|       S|\n",
      "|         13|       0|     3|Saundercock, Mr. ...|  male|              20.0|    0|    0|   8.05|       S|\n",
      "|         14|       0|     3|Andersson, Mr. An...|  male|              39.0|    1|    5| 31.275|       S|\n",
      "|         15|       0|     3|Vestrom, Miss. Hu...|female|              14.0|    0|    0| 7.8542|       S|\n",
      "|         16|       1|     2|Hewlett, Mrs. (Ma...|female|              55.0|    0|    0|   16.0|       S|\n",
      "|         17|       0|     3|Rice, Master. Eugene|  male|               2.0|    4|    1| 29.125|       Q|\n",
      "|         18|       1|     2|Williams, Mr. Cha...|  male| 33.00598190211949|    0|    0|   13.0|       S|\n",
      "|         19|       0|     3|Vander Planke, Mr...|female|              31.0|    1|    0|   18.0|       S|\n",
      "|         20|       1|     3|Masselmani, Mrs. ...|female|25.220669460419217|    0|    0|  7.225|       C|\n",
      "+-----------+--------+------+--------------------+------+------------------+-----+-----+-------+--------+\n",
      "only showing top 20 rows\n",
      "\n",
      "-- test -----\n",
      "['Fare', 'Pclass_C', 'SibSp_C', 'Parch_C', 'Sex_I_C', 'Embarked_I_C']\n",
      "RMSE: 11.659542\n",
      "r2: 0.321974\n",
      "+-----------+------+--------------------+------+----------------+-----+-----+-------+--------+\n",
      "|PassengerId|Pclass|                Name|   Sex|             Age|SibSp|Parch|   Fare|Embarked|\n",
      "+-----------+------+--------------------+------+----------------+-----+-----+-------+--------+\n",
      "|        892|     3|    Kelly, Mr. James|  male|            34.5|    0|    0| 7.8292|       Q|\n",
      "|        893|     3|Wilkes, Mrs. Jame...|female|            47.0|    1|    0|    7.0|       S|\n",
      "|        894|     2|Myles, Mr. Thomas...|  male|            62.0|    0|    0| 9.6875|       Q|\n",
      "|        895|     3|    Wirz, Mr. Albert|  male|            27.0|    0|    0| 8.6625|       S|\n",
      "|        896|     3|Hirvonen, Mrs. Al...|female|            22.0|    1|    1|12.2875|       S|\n",
      "|        897|     3|Svensson, Mr. Joh...|  male|            14.0|    0|    0|  9.225|       S|\n",
      "|        898|     3|Connolly, Miss. Kate|female|            30.0|    0|    0| 7.6292|       Q|\n",
      "|        899|     2|Caldwell, Mr. Alb...|  male|            26.0|    1|    1|   29.0|       S|\n",
      "|        900|     3|Abrahim, Mrs. Jos...|female|            18.0|    0|    0| 7.2292|       C|\n",
      "|        901|     3|Davies, Mr. John ...|  male|            21.0|    2|    0|  24.15|       S|\n",
      "|        902|     3|    Ilieff, Mr. Ylio|  male|26.0066152304355|    0|    0| 7.8958|       S|\n",
      "|        903|     1|Jones, Mr. Charle...|  male|            46.0|    0|    0|   26.0|       S|\n",
      "|        904|     1|Snyder, Mrs. John...|female|            23.0|    1|    0|82.2667|       S|\n",
      "|        905|     2|Howard, Mr. Benjamin|  male|            63.0|    1|    0|   26.0|       S|\n",
      "|        906|     1|Chaffee, Mrs. Her...|female|            47.0|    1|    0| 61.175|       S|\n",
      "|        907|     2|del Carlo, Mrs. S...|female|            24.0|    1|    0|27.7208|       C|\n",
      "|        908|     2|   Keane, Mr. Daniel|  male|            35.0|    0|    0|  12.35|       Q|\n",
      "|        909|     3|   Assaf, Mr. Gerios|  male|            21.0|    0|    0|  7.225|       C|\n",
      "|        910|     3|Ilmakangas, Miss....|female|            27.0|    1|    0|  7.925|       S|\n",
      "|        911|     3|\"Assaf Khalil, Mr...|female|            45.0|    0|    0|  7.225|       C|\n",
      "+-----------+------+--------------------+------+----------------+-----+-----+-------+--------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for t in ['train', 'test']:\n",
    "    print('-- {} -----'.format(t))\n",
    "    df[t] = impute(df[t], 'PassengerId', 'Age', ['Fare'], ['Pclass','SibSp','Parch'], ['Sex', 'Embarked'])\n",
    "    df[t].show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### No NA beyond this point"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "for t in ['train', 'test']:\n",
    "    assert not contains_na(df[t], df[t].columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Write the output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "for t in ['train', 'test']:\n",
    "    engine.write(df[t], '.etl.clean.{}'.format(t), mode='overwrite')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
