version: '2'
services:
  mysql:
    image: mysql
    command: --default-authentication-plugin=mysql_native_password
    volumes:
      - ./volumes/mysql:/var/lib/mysql
    restart: always
    environment:
      MYSQL_ROOT_PASSWORD: root

  redis:
    image: redis
    restart: always
    volumes:
      - ./volumes/redis:/var/lib/redis

  cassandra:
    image: cassandra
    restart: always
    volumes:
      - ./volumes/cassandra:/var/lib/cassandra

  notebook:
    build: jupyter
    command: start.sh jupyter lab
    volumes:
      - ./jupyter/work:/home/jovyan/work
    environment:
      SPARK_OPTS: '--master=spark://spark-master:7077'
    ports:
      - 8888:8888

  zookeeper:
    image: zookeeper
    ports:
      - "2181:2181"
    environment:
      ZOO_MY_ID: 1
      ZOO_SERVERS: server.1=zookeeper:2888:3888

  kafka:
    image: wurstmeister/kafka
    ports:
      - 9092:9092
    environment:
      KAFKA_ADVERTISED_HOST_NAME: kafka
      KAFKA_ADVERTISED_PORT: 9092
      KAFKA_ZOOKEEPER_CONNECT: zookeeper:2181
      KAFKA_CREATE_TOPICS: "datalabframework:1:1"
    volumes:
      - /var/run/docker.sock:/var/run/docker.sock

  elasticsearch:
    build: elasticsearch/
    volumes:
      - ./elasticsearch/config/elasticsearch.yml:/usr/share/elasticsearch/config/elasticsearch.yml
    ports:
      - "9200:9200"
      - "9300:9300"
    environment:
      ES_JAVA_OPTS: "-Xmx256m -Xms256m"

  logstash:
    build: logstash/
    volumes:
      - ./logstash/config/logstash.yml:/usr/share/logstash/config/logstash.yml
      - ./logstash/pipeline:/usr/share/logstash/pipeline
    environment:
      LS_JAVA_OPTS: "-Xmx256m -Xms256m"
    depends_on:
      - elasticsearch

  kibana:
    build: kibana/
    volumes:
      - ./kibana/config/:/usr/share/kibana/config
    ports:
      - "5601:5601"
    depends_on:
      - elasticsearch

  spark-master:
    image: jupyter/pyspark-notebook
    command: /usr/local/spark/bin/spark-class org.apache.spark.deploy.master.Master -h spark-master
    hostname: spark-master
    environment:
      MASTER: spark://spark-master:7077
      SPARK_CONF_DIR: /conf
      SPARK_PUBLIC_DNS: localhost
    ports:
      - 4040
      - 6066
      - 7001
      - 7002
      - 7003
      - 7004
      - 7005
      - 7006
      - 7077
      - 8080:8080
    volumes:
      - ./spark/conf/master:/conf
      - ./volumes/spark/master/data:/tmp/data

  spark-worker:
    image: jupyter/pyspark-notebook
    command: /usr/local/spark/bin/spark-class org.apache.spark.deploy.worker.Worker spark://spark-master:7077
    hostname: spark-worker
    environment:
      SPARK_CONF_DIR: /conf
      SPARK_WORKER_CORES: 2
      SPARK_WORKER_MEMORY: 2g
      SPARK_WORKER_PORT: 8881
      SPARK_WORKER_WEBUI_PORT: 8081
      SPARK_PUBLIC_DNS: localhost
    ports:
      - 7012
      - 7013
      - 7014
      - 7015
      - 7016
      - 8081:8081
    volumes:
      - ./spark/conf/worker:/conf
      - ./volumes/spark/worker/data:/tmp/data
      - ./volumes/spark/worker/work:/usr/local/spark/work

  hdfs-nn:
    image: itrust/hdfs:2.7.1
    hostname: hdfs-nn
    command: /run-namenode.sh
    volumes:
      - ./volumes/hdfs/namenode:/hadoop/dfs/name
    environment:
      - CLUSTER_NAME=test
      - CORE_CONF_fs_defaultFS=hdfs://hdfs-nn:8020
      - CORE_CONF_hadoop_http_staticuser_user=root
      - CORE_CONF_hadoop_proxyuser_hue_hosts=*
      - CORE_CONF_hadoop_proxyuser_hue_groups=*
      - HDFS_CONF_dfs_webhdfs_enabled=true
      - HDFS_CONF_dfs_permissions_enabled=false
    ports:
      - 50070:50070

  hdfs-dn:
    image: itrust/hdfs:2.7.1
    links:
        - hdfs-nn
    command: /run-datanode.sh
    volumes:
      - ./volumes/hdfs/datanode:/hadoop/dfs/data
    environment:
      - CORE_CONF_fs_defaultFS=hdfs://hdfs-nn:8020
      - CORE_CONF_hadoop_http_staticuser_user=root
      - CORE_CONF_hadoop_proxyuser_hue_hosts=*
      - CORE_CONF_hadoop_proxyuser_hue_groups=*
      - HDFS_CONF_dfs_webhdfs_enabled=true
      - HDFS_CONF_dfs_permissions_enabled=false
